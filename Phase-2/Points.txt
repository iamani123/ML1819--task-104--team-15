1) In case the amount of data is huge, pre-processing like standardization helps in improving the accuracy.
2) value of C for Logistic was giving best accuracy at 10 when iterated in increment of 10x between 0.001 to 1000, penalty l1 is performing better than l2
3) value of C is 1.0 having same reason.
4) for kNN value of k = 10 and p = 2 (euclidean distance when taking metric = minkowski)
5) for decision tree value of max depth was found to be 4 when iterated over 1 to 100
6) for random forest - n_estimators and max_depth was taken as 10, after performing iteration from 1 to 100 for each hyperparameter.
7)  increasing the amount of data improves every algorithm's generalization error. But, for large amounts of data, the improvements start becoming negligible. 
And unless the algorithm is very bad, they all settle down to some accuracy level.
8) Better data doesn't mean more data. With better data we have more accuracy but with just more data we can see that that the accuracy breaks even after a particular threshold.
9) Data is important but data without a good approach in training machine learning algorithm becomes noise.
on original dataset
10) X1-X5 deleted after feature selection by finding out the importance for LetterRecognition dataset.
11)X2,X3,X4,X8-X11 for CreditCard dataset.
12) Ranking feature by importance aids in feature selection.
Reference : https://ieeexplore-ieee-org.elib.tcd.ie/stamp/stamp.jsp?tp=&arnumber=8258006



